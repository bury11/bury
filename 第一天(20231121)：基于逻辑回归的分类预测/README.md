# 一、学习知识点概要


# 二、学习内容
## 原理内容
### 逻辑斯蒂回归和线性回归
在线性回归（感知机）中，我们知道一个分离超平面 $w \cdot x\$ 将特征空间分成两个部分，实例在不同的子空间中则被分为相对应的类。但是线性回归的一个问题在于，我们不知道一个新输入的实例，它属于一个类的概率是多少。  
  
换句话说，新输入实例在特征空间中的位置可能与分离超平面距离非常近，也有可能非常远，如果距离较远，那么它更有可能被分成它所在一侧对应的类，但是如果与超平面的距离非常近，说明它被分成另一类的可能性也很大，比如被分成A的可能性为51%，而分成B类的可能性为49%，此时线性回归会将其分为A类，而忽略了49%分成B类的可能性，也就是说，线性回归仅给出结论，未给出概率。

于是，为了得到这一概率，我们引入了Sigmoid函数：

$$
sigmoid(x) = \frac{1}{{1 + {e^{ - x}}}}\
$$

 $Sigmoid\$ 函数能够将线性回归产生的值 $(-∞,+∞)\$ 转换到(0,1)区间内，而概率的取值也在(0,1)内，这样，就可以显示一个实例被分为一个类的概率是多少了。
### 二项逻辑斯谛回归模型{#head3}
首先来看逻辑斯蒂函数的一般形式，其分布具有以下分布函数和密度函数：

$$
F(x) = P(X \le x) = \frac{1}{{1 + {e^{ - (x - \mu )/\gamma }}}}\
$$

$$
f(x) = F'(x) = \frac{1}{{{{(1 + {e^{ - (x - \mu )/\gamma }})}^2}}} \times {e^{ - (x - \mu )/\gamma }} \times \frac{1}{\gamma } = \frac{{{e^{ - (x - \mu )/\gamma }}}}{{\gamma {{(1 + {e^{ - (x - \mu )/\gamma }})}^2}}}\
$$

式中，μ为位置参数，γ>0为形状参数。

![逻辑斯蒂分布的密度函数与分布函数](https://github.com/bury11/bury/blob/main/%E7%AC%AC%E4%B8%80%E5%A4%A9(20231121)%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E4%B8%8E%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0.png?raw=true)

分布函数以 $(μ,1/2)\$ 为中心对称，满足：

$$
F( - x + \mu ) - \frac{1}{2} =  - F(x + \mu ) + \frac{1}{2}\
$$

形状参数γ的值越小，分布函数曲线在中心附近增长得越快。

现在，我们让 $μ\$ 取0， $γ\$ 取1，即得到我们在逻辑斯蒂回归中使用的函数：

$$
F(x) = \frac{1}{{1 + {e^{ - x}}}} = \frac{{{e^x}}}{{1 + {e^x}}}\
$$

采用上式，我们将线性回归产生的值代入到 $Sigmoid\$ 函数之中，可得：

$$
\{\rm{P}}(Y = 1|x) = \frac{{\exp (w \cdot x + b)}}{{1 + \exp (w \cdot x + b)}}\
$$

$$
\{\rm{P}}(Y = 0|x) = \frac{1}{{1 + \exp (w \cdot x + b)}}\
$$


二项逻辑斯谛回归模型是一种分类模型，由条件概率分布 $P(Y|x)\$ 表示。这里， $x \in {R^n}\$ 是输入, $Y \in \{ {0,1}\}\$ 是输出， $w \in {R^n}\$ 和 $b \in R\$ 是参数， $w\$ 称为权值向量， $b\$ 称为偏置， $w \cdot x\$ 为 $w\$ 和 $x\$ 的内积。 

对于给定的输入实例 $x\$ ，按照式(1)和式(2)可以求得 $\{\rm{P}}(Y = 1|x)\$ 和 $\{\rm{P}}(Y = 0|x)\$ 。逻辑斯谛回归比较两个条件**概率值**的大小，将实例 $x\$ 分到概率值较大的那一类。这样，我们就将范围为实数的线性回归产生的值转变为逻辑斯蒂回归中仅在(0,1)范围之内。

有时为了方便，将权值向量和输入向量加以扩充，仍记作 $w,x\$ ,即 $w = {({w^{(1)}},{w^{(2)}},...,{w^{(n)}},b)^T},x = {({x^{(1)}},{x^{(2)}},...,{x^{(n)}},1)^T}\$ 。注意扩充时将最后一项分别设为 $b,1\$ 

这时，逻辑斯谛回归模型如下:

$$
\{\rm{P}}(Y = 1|x) = \frac{{\exp (w \cdot x )}}{{1 + \exp (w \cdot x )}}\tag{1}
$$

$$
\{\rm{P}}(Y = 0|x) = \frac{1}{{1 + \exp (w \cdot x )}}\tag{2}
$$

一个事件的几率是指该事件发生在概率与该事件不发生的概率的比值。如果事件发生的概率是 $p\$ ，那么该事件的对数几率或 $logit\$ 函数是

$$
\log it(p) = \log \frac{p}{{1 - p}}\
$$

对于逻辑斯蒂回归而言，由式 $(1)\$ 和式 $(2)\$ 可得：

$$
\frac{{P(Y = 1|x)}}{{1 - P(Y = 1|x)}} = {e^{w \cdot x}}\
$$

$$
\log (\frac{{P(Y = 1|x)}}{{1 - P(Y = 1|x)}}) = w \cdot x\
$$

通过上式我们知道，通过几率的概念对线性函数进行转换，可以得到逻辑斯蒂回归公式。

一个直观的理解是，对于上式，分子是y=1的概率，而分母是y≠1的概率，显然 $w \cdot x\$ 越大，y=1的概率越大，也就是实例点x在y=1的一侧距离分离超平面越远，则y=1的概率越大。

### 二元逻辑回归的损失函数
回顾下线性回归的损失函数，由于线性回归是连续的，所以可以使用模型误差的的平方和来定义损失函数。但是逻辑回归不是连续的，自然线性回归损失函数定义的经验就用不上了。不过我们可以用最大似然法来推导出我们的损失函数。

我们知道，按照第二元逻辑回归的定义，假设我们的样本输出是0或者1两类。那么我们有：

$$
P(Y = 1|x) = h(x) = \frac{{{e^{w \cdot x}}}}{{1 + {e^{w \cdot x}}}}\
$$

$$
P(Y = 0|x) = 1 - h(x) = \frac{1}{{1 + {e^{w \cdot x}}}}\
$$

把这两个式子写成一个式子，就是：

$$
P(y|x) = h{(x)^y}{(1 - h(x))^{1 - y}}\
$$

其中y的取值只能是0或者1。

得到了y的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数 $w\$ 。

为了方便求解，这里我们用对数似然函数最大化解出参数的估计，对数似然函数取反即相我们的损失函数 $J(w)\$ ，对数似然函数取反最小化相当于损失函数最小化。

其中似然函数的代数表达式为： $L(w) = \prod\limits_{i = 1}^m {{{(h({x_i}))}^{{y_i}}}{{(1 - h({x_i}))}^{1 - {y_i}}}} \$ 
，其中m为样本的个数。

对似然函数对数化的表达式为：

$$
\begin{align*}
\ln L(w) &= \sum_{i = 1}^m [{y_i}\log h({x_i}) + (1 - {y_i})\log (1 - h({x_i}))] \\
         &= \sum_{i = 1}^m [{y_i}\log \frac{{h({x_i})}}{{1 - h({x_i})}} + \log (1 - h({x_i}))] \\
         &= \sum_{i = 1}^m [{y_i}(w \cdot {x_i}) + \log (1 - \frac{{{e^{w \cdot x}}}}{{1 + {e^{w \cdot x}}}})] \\
         &= \sum_{i = 1}^m [{y_i}(w \cdot {x_i}) - \log (1 + {e^{w \cdot x}})] 
\end{align*}
$$

对L(w)求极大值，得到w的估计值。这样，问题就变成了以对数似然函数为目标函数的最优化问题。逻辑斯谛回归学习中通常采用的方法是梯度下降法及拟牛顿法。
我们用矩阵法表示对似然函数对数化取反的表达式，即损失函数表达式为： $J(w) =  - \ln L(w) =  - {Y^T}\log h(X) - {(E - Y)^T}\log (E - h(X))\$ 

### 二元逻辑回归的损失函数的优化方法

对于二元逻辑回归的损失函数极小化，有比较多的方法，最常见的有梯度下降法，坐标轴下降法，拟牛顿法等。这里推导出梯度下降法中每次迭代的公式。由于代数法推导比较的繁琐，故用矩阵法来做损失函数的优化过程，这里给出矩阵法推导二元逻辑回归梯度的过程。

对于 $J(w) =  - \ln L(w) =  - {Y^T}\log h(X) - {(E - Y)^T}\log (E - h(X))\$ ，我们用 $J(w)\$ 对 $w\$ 向量求导可得：

$$
\frac{\partial }{{\partial w}}J(w) = {X^T}(h(X) - Y)\
$$

从而在梯度下降法中每一步向量 $w\$ 的迭代公式如下：

$$
w = w - \alpha {X^T}(h(X) - Y)\
$$

其中， $α\$ 为梯度下降法的步长。  
实践中，我们一般不用操心优化方法，大部分机器学习库都内置了各种逻辑回归的优化方法，不过了解至少一种优化方法还是有必要的。

### 二元逻辑回归的正则化

逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有L1正则化和L2正则化。

逻辑回归的L1正则化的损失函数表达式如下，相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数 $α\$ 作为惩罚系数，调节惩罚项的大小。

二元逻辑回归的L1正则化损失函数表达式如下：

$$
J(w) =  - {Y^T} \cdot \log h(X) - {(E - Y)^T} \cdot \log (E - h(X)) + \alpha ||w|{|_1}\
$$

其中 $||w|{|_1}\$ 为 $w\$ 的L1范数

逻辑回归的L1正则化损失函数的优化方法常用的有坐标轴下降法和最小角回归法。

二元逻辑回归的L2正则化损失函数表达式如下：

$$
J(w) =  - {Y^T} \cdot \log h(X) - {(E - Y)^T} \cdot \log (E - h(X)) + \frac{1}{2}\alpha ||w||_2^2\
$$

其中 $||w|{|_2}\$ 为 $w\$ 的L2范数
逻辑回归的L2正则化损失函数的优化方法和普通的逻辑回归类似。
# 三、学习问题与解答

### 为什么二项逻辑回归模型中用wx而不是wx+b？
* 因为在将权值向量和输入向量加以扩充时，虽然记作 $w,x\$ ,但 $w = {({w^{(1)}},{w^{(2)}},...,{w^{(n)}},b)^T},x = {({x^{(1)}},{x^{(2)}},...,{x^{(n)}},1)^T}\$ 。
注意到扩充时将最后一项分别设为 $b,1\$ ，所以模型中使用的是wx而不是wx+b
### 将多个二分类的逻辑回归组合，即可实现多分类，怎么组合？

# 四、学习思考与总结


逻辑回归从其原理上来说，逻辑回归其实是实现了一个决策边界：对于函数 $y = \frac{1}{{1 + {e^{ - x}}}}\$ ，当 $z \le 0\$ 时， $y \ge 0.5\$ ，分类为1，当 $z<0\$ 时， $yz<0.5\$ ，分类为0，其对应的 $y\$ 值我们可以视为类别1的概率预测值。  
而对于多分类而言，将多个二分类的逻辑回归组合，即可实现多分类。
