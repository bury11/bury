# 最大熵模型
## 最大熵原理

***熵的定义***  
设 $X\$ 是一个取有限个值的离散随机变量，其概率分布为

$$
{\rm{P}}(X = {x_i}) = {p_i},  i = 1,2,...,n\
$$

则随机变量 $X\$ 的熵定义为

$$
H(X) =  - \sum\limits_{i = 1}^n {{p_i}} \log {p_i}\tag{1}
$$

在式(1)中，若 ${p_i} = 0\$ ，则定义 $0log0=0\$ 。通常其中对数以2为底或以e为底，这时熵的单位分别被称为比特(bit)，或纳特(nat)。

***条件熵***  
设有随机变量 $H(X,Y)\$ 表示在已知随机变量 $X\$ 的条件下随机变量Y的不确定性。  
随机变量 $X\$ 给定的条件下随机变量 $Y\$ 的条件熵(conditional entropy) $H(Y|X)\$ ，定义为 $X\$ 给定条件下 $Y\$ 的条件概率分布的熵对 $X\$ 的数学期望

$$
H(Y|X) = E{p_i}(H(Y|X = {x_i})) = \sum\limits_{i = 1}^n {{p_i}H(Y|X = {x_i})}  = \sum\limits_{i = 1}^n {P(X = {x_i})H(Y|X = {x_i})} \
$$

这里， ${p_i} = P(X = {x_i}), i = 1,2, \cdots ,n\$ 。

***最大熵原理***  
由定义可知，熵只依赖于 $X\$ 的分布，而与 $X\$ 的取值无关，所以也将 $X\$ 的熵记作 $H(p)\$ ，即

$$
H(p) =  - \sum\limits_{i = 1}^n {{p_i}} \log {p_i}\
$$

熵越大，随机变量的不确定性就越大。从定义可验证

$$
0 \le H(p) \le \log n\
$$

式中， $n\$ 时 $X\$ 的取值个数，当且仅当 $X\$ 的分布是均匀分布时右边等号成立。这也就是说，当 $X\$ 服从均匀分布时，熵最大。

推导：  
求熵的最大值即求 $\max  H =  - \sum\limits_{i = 1}^n {{p_i}} \log {p_i} + \lambda (\sum\limits_{i = 1}^n {{p_i}}  - 1)\$ ，其中 $\lambda (\sum\limits_{i = 1}^n {{p_i}}  - 1)\$ 为 $H\$ 的约束条件，这个约束条件由 $\sum\limits_{i = 1}^n {{p_i}}  = 1\$ 得来

我们使用拉格朗日乘数法对每个 ${p_i}\$ 求偏导可得同样的结果

$$
-(\log {p_i} + 1) + \lambda  = 0\
$$

$$
{p_i} = {2^{\lambda  - 1}}\
$$

又因为 $\sum\limits_{i = 1}^n {{p_i}}  = 1\$ ，故 $\sum\limits_{i = 1}^n {{p_i}}  = n \cdot {2^{\lambda  - 1}} = 1\$ ，由此可得 $p_i = \frac{1}{n}\$ 

即 $0 \le H(p) \le \log n\$ 
连续分布中正态分布熵最大，具体推导过程见[b站简博士](https://www.bilibili.com/video/BV1No4y1o7ac?p=65&vd_source=17517435653aa14cfea6edfa3d9b5f96)  
**举例**  
假设X有5个取值，满足以下约束条件（先验信息）： $P(A)+P(B)+P(C)+P(D)+P(E)=1\$   
满足这个约束条件的概率分布有无穷多个,在没有任何其他信息的情况下，我们还是需要对概率分布进行估计，此时我们可以认为这个分布中取各个值的概率是相等的，即 $P(A)=P(B)=P(C)=P(D)=P(E)=1/5\$ ，同样也是因为没有其它的信息（约束条件），因此等概率的判断是合理的。

直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性。“等可能”不容易操作，而熵则是一个可优化的数值指标。
## 最大熵模型的定义

最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。


假设分类模型是一个条件概率分布 $P(Y|X)\$ ， $X\$ 表示输入， $Y\$ 表示输出。这个模型表示的是对于给定的输入 $X\$ ，以条件概率 $P(Y|X)\$ 输出 $Y\$ 。
给定一个训练数据集：
$T = \{ ({x_1},{y_1}),({x_2},{y_2}),...,({x_N},{y_N})\} \$ 
，学习的目标是用最大熵原理选择最好的分类模型。  

我们可以首先考虑模型应该满足的条件，即约束条件。给定训练数据集，可以确定联合分布P(X,Y）的经验分布和边缘分布P(X）的经验分布，两者都可以通过训练集算出来，分别以 $\widetilde{P}(X,Y)$ 和 $\widetilde{P}(X)$ 表示。

$$
\widetilde P(X = x,Y = y) = \frac{{v(X = x,Y = y)}}{N}\
$$

$$
\widetilde P(X = x) = \frac{{v(X = x)}}{N}\
$$

其中， $v(X = a,Y =y)\$ 表示训练数据中样本 $(x, y)\$ 出现的频数， $v(X = x)\$ 表示训练数据中输入 $x\$ 出现的频数， $N\$ 表示训练样本容量。

用特征函数(feature function)  $f(x, y)\$ 描述输入 $x\$ 和输出 $y\$ 之间的某一个事实。其定义是

$$
 f(x,y) =
  \begin{cases}
    1,       & \quad \text{x and y satisfy a certain fact}\\
    0,  & \quad \text{otherwise}
  \end{cases}
\
$$

它是一个二值函数，当 $x\$ 和 $y\$ 满足这个事实时取值为1，否则取值为0。

特征函数 $f(x,y)$ 关于经验分布 $\widetilde{P}(X,Y)$ 的期望值，用 $E_{\widetilde{P}}(f)$ 表示:

$$
{E_{\widetilde P}}(f) = \sum\limits_{x,y} {\widetilde P(x,y)f(x,y)} \
$$

特征函数 $f(x,y)\$ 关于模型 $P(Y|X)\$ 与经验分布 $\widetilde{P}(X)$ 的期望值，用 $E_{{P}}(f)$ 表示:

$$
{E_P}(f) = \sum\limits_{x,y} {P(x,y)f(x,y)} = \sum\limits_{x,y} {P(x)P(y|x)f(x,y)} = \sum\limits_{x,y} {\widetilde P(x)P(y|x)f(x,y)}\
$$

*我们的目的是用最大熵原理选择最好的分类模型，也就是选择一个条件概率分布 $P(y|x)\$ ，所以我们用 $x\$ 的经验分布 $\widetilde P(x)\$ 作为它的分布 $P(x)\$ 估计值将其替代*

如果模型能够获取训练数据中的信息(模型预测结果与训练数据结果一致)，那么就可以假设这两个期望值相等，即

$$
{E_P}(f) = {E_{\widetilde P}}(f)\tag{2}
$$

或

$$
\sum\limits_{x,y} {\widetilde P(x)P(y|x)f(x,y)}  = \sum\limits_{x,y} {\widetilde P(x,y)f(x,y)} \tag{3}
$$

确定对于确定x和y的条件熵为：

$$

$$


